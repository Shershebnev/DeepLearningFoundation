{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:38, 4.39MB/s]                              \n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    lower_bound = 0\n",
    "    higher_bound = 1\n",
    "    max_val = np.max(x)\n",
    "    min_val = np.min(x)\n",
    "    return min_val + (higher_bound - lower_bound) * (x - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "label_map = {i : i for i in range(0, 10)}\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    one_hot = np.zeros((len(x), len(label_map)))\n",
    "    for i in range(len(x)):\n",
    "        one_hot[i, label_map[x[i]]] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(shape = [None, *image_shape], dtype = tf.float32, name = \"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(shape = [None, n_classes], dtype = tf.float32, name = \"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(dtype = tf.float32, name = \"keep_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bottleneck(x_tensor, bn_size):\n",
    "    \"\"\"\n",
    "    Returns a bottleneck block\n",
    "    : x_tensor: TF tensor\n",
    "    : bn_size: bottleneck size\n",
    "    : return: bottleneck tensor\n",
    "    \"\"\"\n",
    "    def _layer(tensor, weight_shape, bias_shape, strides, padding):\n",
    "        weights = tf.Variable(tf.truncated_normal(weight_shape, stddev = 0.1))\n",
    "        bias = tf.Variable(tf.truncated_normal(bias_shape, stddev = 0.1))\n",
    "        layer = tf.nn.conv2d(tensor, weights, strides = strides, padding = padding)\n",
    "        layer = tf.nn.bias_add(layer, bias)\n",
    "        layer = tf.nn.elu(layer)\n",
    "        return layer\n",
    "    # 1x1 bn_size, 3x3 bn_size, 1x1 x_tensor_size\n",
    "    depth = x_tensor.get_shape().as_list()[-1]\n",
    "    conv = _layer(x_tensor, [1, 1, depth, bn_size], [bn_size], [1, 1, 1, 1], \"VALID\")\n",
    "    conv = _layer(conv, [3, 3, bn_size, bn_size], [bn_size], [1, 1, 1, 1], \"SAME\")\n",
    "    conv = _layer(conv, [1, 1, bn_size, depth], [depth], [1, 1, 1, 1], \"VALID\")\n",
    "    tensor = conv + x_tensor\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x_tensor, conv_num_outputs, conv_ksize, conv_strides):\n",
    "    depth = x_tensor.get_shape().as_list()[-1]\n",
    "    weight = tf.Variable(tf.truncated_normal([*conv_ksize, depth, conv_num_outputs], stddev = 0.1))\n",
    "    bias = tf.Variable(tf.truncated_normal([conv_num_outputs], stddev = 0.1))\n",
    "    layer = tf.nn.conv2d(x_tensor, weight, strides = [1, *conv_strides, 1], padding = \"SAME\")\n",
    "    layer = tf.nn.bias_add(layer, bias)\n",
    "    layer = tf.nn.elu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    b, h, w, d = x_tensor.get_shape().as_list()\n",
    "    return tf.reshape(x_tensor, [-1, h * w * d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight = tf.Variable(tf.truncated_normal([x_tensor.get_shape().as_list()[1], num_outputs], stddev = 0.1), name = \"conn_weight\")\n",
    "    bias = tf.Variable(tf.truncated_normal([num_outputs], stddev = 0.1), name = \"conn_bias\")\n",
    "    layer = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    layer = tf.nn.elu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight = tf.Variable(tf.truncated_normal([x_tensor.get_shape().as_list()[1], num_outputs], stddev = 0.1), name = \"output_weight\")\n",
    "    bias = tf.Variable(tf.truncated_normal([num_outputs], stddev = 0.1), name = \"output_bias\")\n",
    "    return tf.add(tf.matmul(x_tensor, weight), bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 1024\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "resnet = conv2d(x, 64, [7, 7], [1, 1])\n",
    "resnet = tf.nn.max_pool(resnet, ksize = [1, 3, 3, 1], strides = [1, 2, 2, 1],\n",
    "                        padding = \"SAME\")\n",
    "resnet = conv2d(resnet, 256, [1, 1], [1, 1])\n",
    "for _ in range(3):\n",
    "    resnet = bottleneck(resnet, 64)\n",
    "# upscale\n",
    "resnet = conv2d(resnet, 512, [1, 1], [1, 1])\n",
    "for _ in range(4):\n",
    "    resnet = bottleneck(resnet, 128)\n",
    "# upscale\n",
    "resnet = conv2d(resnet, 1024, [1, 1], [1, 1])\n",
    "for _ in range(6):\n",
    "    resnet = bottleneck(resnet, 256)\n",
    "# upscale\n",
    "resnet = conv2d(resnet, 2048, [1, 1], [1, 1])\n",
    "for _ in range(3):\n",
    "    resnet = bottleneck(resnet, 512)\n",
    "resnet_shape = resnet.get_shape().as_list()\n",
    "resnet = tf.nn.avg_pool(resnet, ksize = [1, resnet_shape[1], resnet_shape[2], 1], strides = [1, 1, 1, 1], padding = \"SAME\")\n",
    "resnet = flatten(resnet)\n",
    "resnet = fully_conn(resnet, 32)\n",
    "resnet = tf.nn.dropout(resnet, keep_prob)\n",
    "logits = output(resnet, 10)\n",
    "\n",
    "logits = tf.identity(logits)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        n_batches = 5\n",
    "        counter = 0\n",
    "        start = time.time()\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                if counter % 100 == 0:\n",
    "                    end = time.time() - start\n",
    "                    print(\"Ran {} times in {:.2f} seconds, {:.2f} seconds per ran on average\".format(counter, end, end / 100))\n",
    "                    start = time.time()\n",
    "                sess.run(optimizer, feed_dict = {x: batch_features, y: batch_labels, keep_prob: keep_probability})\n",
    "                counter += 1\n",
    "        loss = sess.run(cost, feed_dict = {x: batch_features, y: batch_labels, keep_prob: 1.0})\n",
    "        acc = sess.run(accuracy, feed_dict = {x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "        print(\"Epoch {:>2}, CIFAR10 BATCH {}:\\nloss is {:.6f}, validation accuracy is {:.6f}\".format(epoch + 1, batch_i, loss, acc))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
